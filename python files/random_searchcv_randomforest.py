# -*- coding: utf-8 -*-
"""Random_searchCV_randomforest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/vraj1231/6bbe681740264aed2b77c6d7a0537c40/random_searchcv_randomforest.ipynb

### Data Loading
"""

from google.colab import drive #using google drive for storage
drive.mount('/content/gdrive')

import seaborn as sns #all libraries 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score

dataset = pd.read_csv("gdrive/My Drive/main.csv")
dataset.head()

# df_train, df_test = train_test_split(dataset, test_size=0.6)
# df_train.shape

# updated_dataset = dataset.drop([ "Unnamed: 0","delivery_date", "payment_datetime"], axis =1)

"""### Standardilization """

# from sklearn.preprocessing import MinMaxScaler
  
# """ MIN MAX SCALER """
  
# min_max_scaler = MinMaxScaler(feature_range =(0, 1))

# print(updated_dataset.columns)
  
# #Scaled feature
# scaled_dataset = pd.DataFrame(min_max_scaler.fit_transform(updated_dataset))
# scaled_dataset.columns = updated_dataset.columns
# scaled_dataset
  
#Standardisation 
  
# Standardisation = preprocessing.StandardScaler()
  
# #Scaled feature
# x_after_Standardisation = Standardisation.fit_transform(x)
  
# print ("\nAfter Standardisation : \n", x_after_Standardisation)

# df_train.columns

"""### Features and Labels of Dataset"""

features = dataset.drop([ "Unnamed: 0","delivery_date", "payment_datetime", "actual_delivery_time", "seller_id","record_number", "item_price", "quantity",'weight', "weight_units", "shipping_fee"], axis =1)
target = dataset["actual_delivery_time"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train , y_test = train_test_split(features ,target, test_size= 0.2)

for col, i in enumerate(X_train):
    print( "\033[1m" + f'{X_train.columns[col]} has {X_train[i].isna().sum()} missing values')

for col, i in enumerate(X_test):
    print( "\033[1m" + f'{X_test.columns[col]} has {X_test[i].isna().sum()} missing values')

"""### Random-Forest-Regresssor"""

from sklearn.ensemble import RandomForestRegressor

random_forest = RandomForestRegressor(n_estimators=200, max_depth = 30, random_state = 42, n_jobs= -1, min_samples_leaf= 4, min_samples_split= 2)

rforest = random_forest.fit(X_train, y_train)

y_pred = rforest.predict(X_test)

y_pred.shape

rmse = np.sqrt( mean_squared_error(y_test, y_pred))
rmse

rforest.score(X_test, y_test)

mean_absolute_error(y_test, y_pred)

rforest.decision_path

from sklearn.metrics import r2_score

r2_score = r2_score(y_test, y_pred)

r2_score

# features = scaled_dataset.drop(["actual_delivery_time", "seller_id","record_number", "item_price", "quantity",'weight', "weight_units", "shipping_fee"], axis =1)
# target = scaled_dataset["actual_delivery_time"]

"""### KNeighbors Regressor"""

from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor

knn = KNeighborsRegressor(n_neighbors=5)  
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

print('Root Mean Squared Error for knn:', np.sqrt(mean_squared_error(y_test, y_pred)))

mean_squared_error(y_test, y_pred)

from sklearn.metrics import mean_absolute_error


mean_absolute_error(y_test,y_pred)

r2_score(y_test, y_pred)

knn.score(X_test, y_test)

"""### Decision Tree Regressor

"""

dt = DecisionTreeRegressor()  
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
print('Root Mean Squared Error for DecisionTree:', np.sqrt(mean_squared_error(y_test, y_pred_dt)))

mean_squared_error(y_test, y_pred_dt)

mean_absolute_error(y_test, y_pred_dt)

r2_score(y_test, y_pred_dt)

"""### XGBoost Regressor"""

import xgboost as xgb

xgb_model = xgb.XGBRegressor(objective="reg:linear", random_state=42)

xgb_model.fit(X_train, y_train)

y_pred_xgb = xgb_model.predict(X_test)

mse=mean_squared_error(y_test, y_pred_xgb)

print(np.sqrt(mse))

mean_absolute_error(y_test, y_pred_xgb)

r2_score(y_test, y_pred_xgb)
print(np.sqrt(mse))

"""### CatBoost Regressor"""

# pip install catboost

from catboost import CatBoostRegressor


cat_model=CatBoostRegressor()
cat_model.fit(X_train, y_train)

y_pred = cat_model.predict(X_test)

mse=mean_squared_error(y_test, y_pred)

print(mse)

mse

pred = cat_random_search.predict(X_test)
mse_cat = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)
r2

mse_cat

mean_absolute_error(y_test, pred)

"""### Fine-tuning Hyperparameters"""

# from sklearn.model_selection import RandomizedSearchCV
# # Number of trees in random forest
# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 3)]
# # Number of features to consider at every split
# # Maximum number of levels in tree
# max_depth = [int(x) for x in np.linspace(10, 50, num = 3)]
# max_depth.append(None)
# # Minimum number of samples required to split a node
# min_samples_split = [2, 5, 10]
# # Minimum number of samples required at each leaf node
# min_samples_leaf = [1, 2, 4]
# # Method of selecting samples for training each tree
# # Create the random grid
# random_grid = {'n_estimators': n_estimators,
#                'max_depth': max_depth,
#                'min_samples_split': min_samples_split,
#                'min_samples_leaf': min_samples_leaf}
# print(random_grid)

# rf = RandomForestRegressor()
# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)

# rf_random.fit(X_train,y_train)

from sklearn.model_selection import RandomizedSearchCV
print("Tuning")

params = {
    # Parameters that we are going to tune.
    'depth':[int(x) for x in np.linspace(start=1, stop=20, num=1)],
    'l2_leaf_reg':[0,1,2,3,4,5,6,7,8,9,10],
    'learning_rate':[0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.01, 0.005],
    'bagging_temperature': [x/100 for x in np.linspace(start=0, stop=100, num=1)],
    'random_strength': [x/100 for x in np.linspace(start=0, stop=100, num=1)],
}

cat_random_search = RandomizedSearchCV(estimator=cat_model,
                                      param_distributions = params,
                                      n_iter = 10,
                                      cv=3,
                                      verbose=2,
                                      random_state=47,
                                      n_jobs=2)

cat_random_search.fit(X_train, y_train)
print(cat_random_search.best_params_)

# rf_random.best_params_

from sklearn.model_selection import train_test_split

X_train, X_test, y_train , y_test = train_test_split(features ,target, test_size= 0.2)

from sklearn.ensemble import RandomForestRegressor

random_forest = RandomForestRegressor(n_estimators=200, max_depth = 30, random_state = 42, n_jobs= -1, min_samples_leaf= 4, min_samples_split= 2)

rforest = random_forest.fit(X_train, y_train)

y_pred = rforest.predict(X_test)
print ("R2_score : ",rforest.score(X_test, y_test))
print("MAE : ", mean_absolute_error(y_test, y_pred))
print("MSE : ", mean_squared_error(y_test, y_pred))

import catboost as cb

train_dataset = cb.Pool(X_train, y_train) 
test_dataset = cb.Pool(X_test, y_test)

cat_model_new = CatBoostRegressor()

grid = {'iterations': [100, 150, 200],
        'learning_rate': [0.03, 0.1],
        'depth': [2, 4, 6, 8],
        'l2_leaf_reg': [0.2, 0.5, 1, 3]}
cat_model_new.grid_search(grid, train_dataset)

cat_model_new.get_all_params

from sklearn.metrics import r2_score

pred = cat_model_new.predict(X_test)
mse = mean_squared_error(y_test, pred)
r2 = r2_score(y_test, pred)
r2

"""### Ensemble Regression VotingRegressor"""

from sklearn.ensemble import VotingRegressor


er = VotingRegressor([('xgboost', xgb_model), ('rf', rforest), ('catboost',cat_model)])
er.fit(X_train, y_train)

y_pred_vr = er.predict(X_test)

mse = mean_squared_error(y_test, y_pred_vr)
r2_score =  r2_score(y_test, y_pred_vr)
print (mse)
print (r2_score)

er_update = VotingRegressor ([ ('rf', rforest), ('catboost',cat_model)])
er_update.fit(X_train, y_train)

y_pred_up = er_update.predict(X_test)
mse = mean_squared_error(y_test, y_pred_up)
r2_score =  r2_score(y_test, y_pred_up)
print (mse)
print (r2_score)