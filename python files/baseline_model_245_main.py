# -*- coding: utf-8 -*-
"""Baseline_model_245_Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hCrms_WRgbpGYCwWzoKMadSKVOrBFwwy

### Mounting drive and loading data
"""

from google.colab import drive #using google drive for storage
drive.mount('/content/gdrive')

import seaborn as sns #all libraries 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import datetime as dt

#converting the file tsv -? csv
# tsv_file= r'C:\Users\mistr\OneDrive\Documents\SJSU studies\DATA 245\project\train.tsv'
# csv_table=pd.read_table(tsv_file,sep='\t')
# csv_table.to_csv('train_ebay.csv',index=False)

#pandas dataframe train and test from google drive 
# dataset_test = pd.read_csv("gdrive/My Drive/quiz.csv")
dataset = pd.read_csv("gdrive/My Drive/train_ebay.csv")
dataset.head()

"""### Data columns """

dataset.describe(include="all") #describe of all features in dataset

#data types of the columns 
dataset.dtypes

dataset = dataset.dropna()

# number of counts for each value
# dataset_train["b2c_c2c"].value_counts()
#  unqiues values for all object types
# for col in dataset_train.select_dtypes(include= "object").columns:
#     print ( f'{dataset_train[col]} : {len (dataset_train[col].unique())}')
# missing value for each columns 
# for col, i in enumerate(dataset_train):
#     print( "\033[1m" + f'{dataset_train.columns[col]} has {dataset_train[i].isna().sum()} missing values')
# heatmap for only numeric values
# corr = dataset_train.corr()

# # plot the heatmap
# sns.heatmap(corr, 
#         xticklabels=corr.columns,
#         yticklabels=corr.columns)
# dataset_train.corr()

"""### Data Cleaning"""

def data_cleaning(dataset_train):
    dataset_train['delivery_date'] = pd.to_datetime(dataset_train['delivery_date'], errors = "coerce", utc=True)
    dataset_train['payment_datetime'] = pd.to_datetime(dataset_train['payment_datetime'], errors = "coerce", utc=True)
    dataset_train['acceptance_scan_timestamp'] = pd.to_datetime(dataset_train['acceptance_scan_timestamp'], errors='coerce', utc=True)
    dataset_train['actual_delivery_time'] = abs ((dataset_train.loc[:,'payment_datetime'] - dataset_train.loc[:,'delivery_date']).dt.days)
    dataset_train['acceptance_scan_timestamp'] = abs ((dataset_train.loc[:,'acceptance_scan_timestamp'] - dataset_train.loc[:,'payment_datetime']).dt.days)
    dataset_train["item_zip"] = pd.to_numeric (dataset_train["item_zip"] , errors = "coerce")
    dataset_train["buyer_zip"] = pd.to_numeric(dataset_train["buyer_zip"], errors = "coerce")
    lab_encoder_package_size = LabelEncoder()
    lab_encoder_package_size.fit(dataset_train["package_size"])
    dataset_train["package_size"] = pd.DataFrame(lab_encoder_package_size.transform(dataset_train["package_size"]))
    lab_encoder_b2c =  LabelEncoder()
    lab_encoder_b2c.fit(dataset_train["b2c_c2c"])
    dataset_train["b2c_c2c"] = pd.DataFrame(lab_encoder_b2c.transform(dataset_train["b2c_c2c"]))
    return dataset_train


dataset_train_updated = data_cleaning(dataset)

#correlations after all columns convertedi into numeric form
 corr = dataset_train_updated.corr()

# plot the heatmap
sns.heatmap(corr, 
        xticklabels=corr.columns,
        yticklabels=corr.columns)

# checking missing value for each columns 
for col, i in enumerate(dataset_train_updated):
    print( "\033[1m" + f'{dataset_train_updated.columns[col]} has {dataset_train_updated[i].isna().sum()} missing values')

#dropping missing values 
dataset_train_updated = dataset_train_updated.dropna()

# confirm missing values 
# for col, i in enumerate(dataset_train_updated):
#     print( "\033[1m" + f'{dataset_train_updated.columns[col]} has {dataset_train_updated[i].isna().sum()} missing values')

# df_train, df_test = train_test_split(dataset_train_updated, test_size=0.6)
# df_train.shape

# df_train.to_csv("main.csv")

# from google.colab import files
# files.download("main.csv")

"""### Features and target data splitting"""

#seperating features and target from the dataset for model prediction
features = df_train.drop(["delivery_date", "payment_datetime", "actual_delivery_time", "seller_id"], axis =1)
target = df_train["actual_delivery_time"]

#80-20 ratio for train and test data on training instances
from sklearn.model_selection import train_test_split

X_train, X_test, y_train , y_test = train_test_split(features ,labels, test_size= 0.2)

"""### Random Forest Regression"""

#using sklearn to build random forest model baseline
from sklearn.ensemble import RandomForestRegressor

random_forest = RandomForestRegressor(n_estimators= 200, max_depth = 30, random_state = 42, n_jobs= -1, min_samples_leaf= 4, min_samples_split= 2)

rforest = random_forest.fit(X_train, y_train)

y_pred = rforest.predict(X_test)

y_pred.shape

#MSE score for the training model
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
mse

"""### Ridge and Lasso"""

from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso

model_ridge = Ridge(alpha= 10, fit_intercept= True)

model_ridge.fit(X_train, y_train)

y_pred_ridge = model_ridge.predict(X_test)

#Mean squared error
mse = mean_squared_error(y_test, y_pred_ridge)
mse

model_lasso = Lasso(alpha= 0.3, fit_intercept=True)

model_lasso.fit(X_train, y_train)

y_pred_lasso = model_lasso.predict(X_test)

#Mean squared error
mse = mean_squared_error(y_test, y_pred_lasso)
mse

"""### Saving the models"""

#saving model
import joblib

joblib.dump(rforest, "my_random_forest_max10_245.joblib")
files.download("my_random_forest_max10_245.joblib")

#loaded_rf = joblib.load("my_random_forest.joblib")   #this is to load